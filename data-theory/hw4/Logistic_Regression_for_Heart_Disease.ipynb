{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will get you started on training a logistic regression model to predict heart disease using historic data. This is a classic exercise from Hastie's *Elements of statistical learning*. I also used parts of the notebook: \n",
    "https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/South%20African%20Heart%20Disease.ipynb\n",
    "\n",
    "in creating this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # useful for reading in data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in. Will need an internet connection!\n",
    "data = pd.read_csv('http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data', error_bad_lines=False)\n",
    "# More info on data can be found at: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first rows. Note that the data points are encoded as rows, with sbp--age being the\n",
    "# components of x and chd (= chronic heart disease) being the response variable, y.\n",
    "# \n",
    "data.head()\n",
    "target = 'chd'\n",
    "features = ['sbp', 'tobacco', 'ldl', 'famhist', 'obesity', 'alcohol', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   row.names  sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  \\\n",
       "0          1  160    12.00  5.73      23.11        1     49    25.30    97.20   \n",
       "1          2  144     0.01  4.41      28.61        0     55    28.87     2.06   \n",
       "2          3  118     0.08  3.48      32.28        1     52    29.14     3.81   \n",
       "3          4  170     7.50  6.41      38.03        1     51    31.99    24.26   \n",
       "4          5  134    13.60  3.50      27.78        1     60    25.99    57.34   \n",
       "\n",
       "   age  chd  \n",
       "0   52    1  \n",
       "1   63    1  \n",
       "2   46    0  \n",
       "3   58    1  \n",
       "4   49    1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row.names</th>\n      <th>sbp</th>\n      <th>tobacco</th>\n      <th>ldl</th>\n      <th>adiposity</th>\n      <th>famhist</th>\n      <th>typea</th>\n      <th>obesity</th>\n      <th>alcohol</th>\n      <th>age</th>\n      <th>chd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>160</td>\n      <td>12.00</td>\n      <td>5.73</td>\n      <td>23.11</td>\n      <td>1</td>\n      <td>49</td>\n      <td>25.30</td>\n      <td>97.20</td>\n      <td>52</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>144</td>\n      <td>0.01</td>\n      <td>4.41</td>\n      <td>28.61</td>\n      <td>0</td>\n      <td>55</td>\n      <td>28.87</td>\n      <td>2.06</td>\n      <td>63</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>118</td>\n      <td>0.08</td>\n      <td>3.48</td>\n      <td>32.28</td>\n      <td>1</td>\n      <td>52</td>\n      <td>29.14</td>\n      <td>3.81</td>\n      <td>46</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>170</td>\n      <td>7.50</td>\n      <td>6.41</td>\n      <td>38.03</td>\n      <td>1</td>\n      <td>51</td>\n      <td>31.99</td>\n      <td>24.26</td>\n      <td>58</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>134</td>\n      <td>13.60</td>\n      <td>3.50</td>\n      <td>27.78</td>\n      <td>1</td>\n      <td>60</td>\n      <td>25.99</td>\n      <td>57.34</td>\n      <td>49</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# The following command Replaces \"Present\" with 1 and \"Absent\" with 0 in the \"famhist\" column\n",
    "data['famhist'] = pd.get_dummies(data['famhist'])['Present']\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(462, 7)\n"
     ]
    }
   ],
   "source": [
    "# Extract the data into numpy arrays\n",
    "X, y = data[features].values, data[target].values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split your data into a train set and a test set here. \n",
    "# You should have 80 data points in your test set.\n",
    "\n",
    "All_indices = range(0,462)\n",
    "Test_indices = np.random.choice(All_indices,size=80,replace=False)\n",
    "Train_indices = np.setdiff1d(All_indices,Test_indices)\n",
    "\n",
    "X_train = X[Train_indices,:]\n",
    "y_train = y[Train_indices]\n",
    "\n",
    "X_test = X[Test_indices,:]\n",
    "y_test = y[Test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your logistic regression code here.\n",
    "def sigma(z):\n",
    "    min_output = 0.00001\n",
    "    max_output = 0.99999\n",
    "    output = 1.0/(1.0+np.exp(-z))\n",
    "    output = max(min_output, output)\n",
    "    output = min(max_output, output)\n",
    "    return output\n",
    "\n",
    "def h(theta,b,x):\n",
    "    return sigma(np.dot(theta,x) + b)\n",
    "\n",
    "def TrainingLogisticRegression(theta_0,b_0,X,y,alpha,K_max):\n",
    "    # Function for training logistic regression model\n",
    "    # Inputs:\n",
    "        # theta_0,b_0 : (random) initializations for parameters\n",
    "        # X,y : labeled training data\n",
    "        # alpha: step size/ learning rate\n",
    "        # K_max: max number of iterations.\n",
    "    theta = np.squeeze(theta_0)\n",
    "    b = b_0\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    loss_function_trajectory = np.zeros([K_max,1])\n",
    "    for k in range(K_max):\n",
    "        theta_grad = 0\n",
    "        b_grad = 0\n",
    "        loss_function_value = 0\n",
    "        for i in range(N):\n",
    "            theta_grad -= (y[i] - h(theta,b,X[i,:]))*X[i,:]\n",
    "            b_grad -= y[i] - h(theta,b,X[i,:])\n",
    "            loss_function_value += -y[i]*np.log(h(theta,b,X[i,:])) -(1-y[i])*np.log(1 - h(theta,b,X[i,:])) \n",
    "        theta -= alpha*theta_grad\n",
    "        b -= alpha*b_grad\n",
    "        loss_function_trajectory[k] = loss_function_value\n",
    "        if k% 50 == 0:\n",
    "            print(loss_function_value)\n",
    "    return theta,b,loss_function_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1542.734492318411\n",
      "<ipython-input-58-bf2f25b30c3b>:5: RuntimeWarning: overflow encountered in exp\n",
      "  output = 1.0/(1.0+np.exp(-z))\n",
      "1542.734492318411\n",
      "1611.8119851082831\n",
      "1807.5315480121692\n",
      "1542.734492318411\n",
      "2452.254814048082\n",
      "2440.7418985831578\n",
      "1439.118253134424\n",
      "[1416.09963916]\n",
      "[1410.48926705]\n"
     ]
    }
   ],
   "source": [
    "# Initializing parameters\n",
    "theta_0 = np.random.randn(7,1)\n",
    "b_0 = np.random.randn(1)\n",
    "alpha = 0.01\n",
    "K_max = 500\n",
    "\n",
    "# training\n",
    "theta,b,loss_function_trajectory = TrainingLogisticRegression(theta_0,b_0,X_train,y_train,alpha,K_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(theta,b,x):\n",
    "    probability = h(theta,b,x) # = sigmoid(theta^{T}x - b)\n",
    "    classification = np.round(probability) # if probability >= 0.5 then predict y = 1. if prob < 0.5 then predict y = 0\n",
    "    return probability,classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-58-bf2f25b30c3b>:5: RuntimeWarning: overflow encountered in exp\n  output = 1.0/(1.0+np.exp(-z))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(80):\n",
    "    prob, y_pred = Classifier(theta, b, X_test[i,:])\n",
    "    if y_pred == y_test[i]:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([343, 338, 206, 280, 412, 185, 184, 382, 417, 331,  55,  80, 120,\n",
       "       202, 212, 165, 409, 400, 155, 221, 391, 450, 453, 194, 149, 128,\n",
       "        44, 287, 127, 164, 416, 251, 177, 198,  91, 259, 352, 393, 357,\n",
       "       174,  38, 447, 279, 152, 364, 356, 271, 307,  93, 408, 375, 398,\n",
       "       124, 101, 161, 418, 383,   9, 407, 431, 323, 210, 413, 229, 146,\n",
       "       377, 261, 226, 317, 414, 126, 457, 189, 285, 183, 215,  13,  89,\n",
       "       336, 459])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "Test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([138.  ,   0.87,   1.87,   0.  ,  26.76,  42.99,  31.  ])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "X_test[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "y[459]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3463203463203463"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "data.chd.sum()/len(data.chd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7125"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "57/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}